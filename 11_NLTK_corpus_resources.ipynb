{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moO0lk/LING227/blob/main/11_NLTK_corpus_resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Relevant readings\n",
        "\n",
        " [NLTK Book, Chapter 2, Section 1](https://www.nltk.org/book/ch02.html)"
      ],
      "metadata": {
        "id": "VeWXOnowfNdG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5cCWAdFYPGL"
      },
      "source": [
        "# NLTK Text Corpora\n",
        "\n",
        "The first section of Chapter 2 in NLTK teaches you how to explore the built-in corpora provided by NLTK. It is important to note that some of the examples used in Chapter 1 of NLTK were pedagogical; the authors have provided us with copora and texts that have already been pre-processed in various ways, or otherwise simplified. While a corpus *is* a large collection of language and documents, a corpus usually also contains metadata telling the user about different categories *within* a corpus. Categories can be anything - genre, speaker, task, etc.\n",
        "\n",
        "In this notebook, we will explore some of the other corpus resources available through the NLTK.\n",
        "\n",
        "We will also look at how you can load your own data into Colab to create your own corpus using NLTK functions.\n",
        "\n",
        "Run the following cell to load in the required resources for this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-j_cbaUYFFt"
      },
      "source": [
        "# import the NLTK library\n",
        "import nltk\n",
        "\n",
        "# download resources for this notebook (takes a bit)\n",
        "nltk_resources = ['gutenberg', 'punkt_tab', 'brown', 'state_union']\n",
        "\n",
        "nltk.download(nltk_resources)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-__SQwtjqt"
      },
      "source": [
        "## The Gutenberg Corpus\n",
        "\n",
        "A good example of a corpus with different categories is the Gutenberg corpus used by NLTK, which is a collection of different public domain books. [Project Gutenberg](https://www.gutenberg.org/) is a website containing thousands of free eBooks, and is named after [Johannes Gutenberg](https://en.wikipedia.org/wiki/Johannes_Gutenberg), associated with the development of the printing press.\n",
        "\n",
        "<img src = https://i.imgur.com/skJBrKl.png height = '200'>\n",
        "\n",
        "\n",
        "The Gutenberg corpus is part of the `nltk.corpus` module, which provides several built-in methods for working with text data. You can see a complete list of the methods here: [NLTK corpus package](https://www.nltk.org/api/nltk.corpus.html). You can also see a breakdown in Table 1.3 in Chapter 2 of the NLTK book.\n",
        "\n",
        "Please note that NLTK includes just a small set of books from Project Gutenberg.  We access the gutenberg data using `nltk.corpus.gutenberg` followed by different NLTK functions and methods.\n",
        "\n",
        "To see the list of all of the files, we use the `.fileids()` method. The filenames are in the form of `author-title.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKGFnvwvZtmB"
      },
      "source": [
        "# inspect the different files in the gutenberg corpus - have you read any of these books?\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsfdJHsjtSgO"
      },
      "source": [
        "Note that the `.fileids()` represent some metadata — the file id contains both the author and the name of the text. So, in this corpus, there are different texts grouped by different authors. As such, *authors* represent the *categories* of this corpus.\n",
        "\n",
        "\n",
        "Chapter 2 introduces you to the methods associated with the NLTK corpus class somewhat implicitly, but it is good to look through all the possibilities. Perhaps the most basic possibility is to use the `.raw()` method to obtain a view of the raw text file.\n",
        "\n",
        "Note that we need to input the fileid of the text we are interested in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLVd9N5iZ6Wn"
      },
      "source": [
        "# we can select a single book using the book's name and the format we want, such as words or sentences.\n",
        "# We use `raw` to get the raw text file (as a string)\n",
        "macbeth = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')\n",
        "\n",
        "# look at the entire text file.\n",
        "macbeth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3QMh4pur91"
      },
      "source": [
        "There are different methods for sorting the corpus into words and sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QBdQfNEuvrY"
      },
      "source": [
        "# the .words method returns words as tokens\n",
        "macbeth_words = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
        "macbeth_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# could we do this on our own?\n",
        "# why do we get different results when we manually split the text?\n",
        "# do you think the .words is using .split(), or nltk.word_tokenize()?\n",
        "nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt').split()[:10]"
      ],
      "metadata": {
        "id": "4odD816HIFaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCTV8JfGu7aY"
      },
      "source": [
        "# use .sents to get the sentences\n",
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BAFCFBWbhhT"
      },
      "source": [
        "Chapter 2 also provides a discussion about how to import Python modules into a shorthand, which saves typing. This helps provide some clarity into why many Python scripts and examples start with lines such as `from x import y as z` — this just means import something and give it a shorter name.\n",
        "\n",
        "For the current example, we can import gutenburg directly, and thus can avoid typing the `nltk.corpus` bit before it. And, you can do this same thing with other functions from other packages / modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG9c7xQdb1Ti"
      },
      "source": [
        "# import the gutenberg package directly\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# now you can access `gutenberg` without needing to type `nltk.corpus`\n",
        "gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGmArcYZ5ZO"
      },
      "source": [
        "## Looping through Gutenberg\n",
        "\n",
        "Chapter 2 includes a demonstration of the different methods NLTK can provide for a raw text by asking you to think about how the following function works.\n",
        "\n",
        "First, look at how the loop works. The loop is over the `fileids()` in `gutenberg.fileids()`, which is just a list of the different text files names. In the body of the loop, the fileid is used to access the specific text - this is why the variable `fileid` is placed inside the brackets for `gutenberg.raw()` and all the other functions (`.words`, `.sents`).\n",
        "\n",
        "Examine the print statement and the output - do you understand how they have controlled the output using this `print()` statement? You might find it useful to include some comments in the code cell below, explaining what each line is doing.\n",
        "\n",
        "The book also claims there are some patterns related to average word length, average sentence length, and lexical diversity for different authors. Do you see these patterns? For instance - who has the longest sentences? The shortest?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3fbTi_pcXO1"
      },
      "source": [
        "# Can you add comments to this code explaining what each line is doing?\n",
        "for fileid in gutenberg.fileids():\n",
        "  num_chars = len(gutenberg.raw(fileid))\n",
        "  num_words = len(gutenberg.words(fileid))\n",
        "  num_sents = len(gutenberg.sents(fileid))\n",
        "  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "  print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Iwz18ssvmNX"
      },
      "source": [
        "## The Brown Corpus\n",
        "\n",
        "You should read through the explanation of various corpora in Chapter 2. One of the more well-known corpora is the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), which has been used in numerous studies of language either for checking word use and collocations, or for compiling frequency statistics of words in spoken and written English. As Chapter 2 explains, the Brown Corpus is an ideal example for how one can use different categories in a corpus to test questions about differences in language use.\n",
        "\n",
        "We load in Brown in a similar manner to Gutenberg. The Brown corpus has a function that Gutenberg did not have: `.categories()`. This function shows different genres or classifications of texts in the Brown corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4MOopTmwFFM"
      },
      "source": [
        "# load in Brown and look at the categories\n",
        "from nltk.corpus import brown\n",
        "\n",
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2Bsh2HxniD"
      },
      "source": [
        "You can select a specific subsection of the brown corpus using `categories = ` when accessing the Brown text as raw, words, sentences, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkj6RNXAxunP"
      },
      "source": [
        "# look at the a sentence from one of the the texts labelled as \"humour\" in the Brown corpus\n",
        "brown.sents(categories = 'humor')[80]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u275-4cpxTu2"
      },
      "source": [
        "### Comparing language among different genres\n",
        "\n",
        "The different genres or categories in Brown allows for a means to compare different writing styles. The NLTK book provide an analysis of [modal verbs](https://en.wikipedia.org/wiki/Modal_verb) as an example.\n",
        "\n",
        "Modal verbs are auxiliary verbs which provide a level of certainty, possibility, or urgency upon a main verb. In English, these are words such as *must*, *will*, *could*, etc.\n",
        "\n",
        "In the following example, the authors of NLTK wrote a function to create a frequency distribution of modal verbs in the `news` category of Brown using the `nltk.FreqDist()` function.\n",
        "\n",
        "Note how they do this. First they define a list of modal words — so they can provide the program with the targets it is trying to find. Then they save the words of the Brown corpus to a new variable `news_text`. Then they create a frequency distribution from a list comprehension which first lowercases the word (i.e., pre-processes it) and then only includes words if they are in the list of modals. Any word *not* in that list of modals will in turn not be included in the resulting Frequency Distribution.\n",
        "\n",
        "Run the cell and then ask yourself, what do you think about the frequency of modal verbs in the `news` category - does it make sense that `will` is the most frequent modal verb for news?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHtHBpbxXAH"
      },
      "source": [
        "# create a frequency distribution for specific modal verbs\n",
        "\n",
        "# define list of modal verbs\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# create an object of words which occur in the news category of brown\n",
        "news_text = brown.words(categories = 'news')\n",
        "\n",
        "# create a frequency distribution - does it make sense for there to be a .lower() here?\n",
        "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
        "\n",
        "# loop through each modal and print the fdist\n",
        "for m in modals:\n",
        "  print(m + ':', fdist[m], end = ' ') # the end argument replaces the default newline which comes at the end of a print statement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Have a play with the Brown corpus, explore the different categories and make sure you are comfortable loading them into Python."
      ],
      "metadata": {
        "id": "z6VoYiYiz4fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State of the Union Corpus\n",
        "\n",
        "Another corpus which provides us some interesting data to use for various comparisons is a set of speeches given by US presidents over the years. Each year, the sitting president gives a \"state of the union\" speech which explains how everything they have done is good and how everything their opponents want to do is bad. Because most US Presidents will serve four or eight subequent years in office, this provides a neat way to compare the speech of different US Presidents over the years.\n",
        "\n",
        "We load in the corpus just like Gutenberg and Brown\n",
        "\n"
      ],
      "metadata": {
        "id": "JdNAP2HW04u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the state_union corpus\n",
        "from nltk.corpus import state_union\n",
        "\n",
        "# fileids shows us the different files\n",
        "state_union.fileids()"
      ],
      "metadata": {
        "id": "ALxYzJ5K1HaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much like Gutenberg, we can access various properties of specific speeches by using the `.raw()`, `.words()`, or `.sents()` methods with a specific fileid in the brackets:"
      ],
      "metadata": {
        "id": "BoumsZq21wgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw words of 1945 speech\n",
        "state_union.raw('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "P-NH6Eq-12PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized version (truncated output)\n",
        "state_union.words('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "AICFNDtu2Nig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences (truncated output)\n",
        "state_union.sents('1945-Truman.txt')"
      ],
      "metadata": {
        "id": "u32PM-Qu2TTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could ask a variety of questions about the nature of State of the Union addresses as they have occurred over time.\n",
        "\n",
        "- Have they changed in length?\n",
        "- What words are similar among all presidents?\n",
        "- What words are unique to different presidents?\n",
        "- Which president is the most lexically diverse?\n",
        "- and so on...\n",
        "\n",
        "Do you have an idea about how to do this? It probably involves some sort of looping over the fileids and then performing a function. For example, I'll write a for loop which reports the most frequent word from each speech. Run the function and then inspect the results. What could you do to get more interesting results? And, what does this function say about the distribution of words in the English language?\n"
      ],
      "metadata": {
        "id": "UGHG_Fsq2oPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in state_union.fileids():\n",
        "  most_frequent_word = nltk.FreqDist(state_union.words(fileid)).most_common(1)\n",
        "  print(f'{fileid} most frequent word is: \\t {most_frequent_word}')"
      ],
      "metadata": {
        "id": "5m2qaqWc3WRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}