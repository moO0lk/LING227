{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moO0lk/LING227/blob/main/08_more_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **More preprocessing**\n",
        "\n",
        "This notebook walks through some more considerations you might want to make when preprocesing text. We have already considered punctuation and tokenizations. Other forms of preprocessing include lowercasing, removing excessive whitespace, and a consideration of so-called stopwords.\n"
      ],
      "metadata": {
        "id": "ik7AfkOxsJdt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhRcumclJek"
      },
      "source": [
        "# **Lexical diversity and preprocessing**\n",
        "\n",
        "Let's start by considering how pre-processing influences the effects of a measure we've already explored: lexical diversity. Compare what capitalization will do to measures of lexical diversity on these two texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkN1UbNQlpiN"
      },
      "source": [
        "# create two texts that only differ based on capitalization\n",
        "version1 = ['Soda', 'soda', 'Onion', 'onion']\n",
        "version2 = ['soda', 'soda', 'onion', 'onion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte their TTR\n",
        "version1_ttr = len(set(version1)) / len(version1)\n",
        "version2_ttr = len(set(version2)) / len(version2)"
      ],
      "metadata": {
        "id": "A3djMe4tC_Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare - version 1 has a higher TTR than version 2, even thoough we see that they are effectively the \"same\" words:"
      ],
      "metadata": {
        "id": "Lo6Cx94yDbqh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fOuhioNl6wF"
      },
      "source": [
        "version1_ttr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPwh7W-Nl8gq"
      },
      "source": [
        "version2_ttr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecwgkFxmFwi"
      },
      "source": [
        "We clearly would not want to think that `version1` is more lexically diverse than `version2`, unless we have strong reason to believe the capitalization results in a fundamentally different word.\n",
        "\n",
        "Hence, normalization is needed to address these issues. If we convert all of the tokens to lowercase, we now obtain the same TTR measures:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use a list comprehension to lowercase all the tokens\n",
        "version1_lower = [token.lower() for token in version1]\n",
        "version2_lower = [token.lower() for token in version2]"
      ],
      "metadata": {
        "id": "YcLsoHaFDqp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate new TTR scores\n",
        "version1_lower_ttr = len(set(version1_lower)) / len(version1_lower)\n",
        "version2_lower_ttr = len(set(version2_lower)) / len(version2_lower)"
      ],
      "metadata": {
        "id": "g_2GizGmDw5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that both texts now have a TTR of 0.5 (50%):"
      ],
      "metadata": {
        "id": "WXNlNuTeEmKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version1_lower_ttr"
      ],
      "metadata": {
        "id": "758oKfM2ERIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "version2_lower_ttr"
      ],
      "metadata": {
        "id": "KXeFTD0EESz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is helpful, we need to be careful. Perhaps in some cases you will want to retain captialization. In English, at least, capitalization will signal names and proper nouns. And in other languages which use logographic scripts, such as Chinese and Japanese, lowercasing does not matter at all!"
      ],
      "metadata": {
        "id": "QgLFEOn2EdFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning whitespace**\n",
        "\n",
        "Text is messy and sometimes there are extra spaces that should be removed from texts. The built-in string functions to do so are:\n",
        "\n",
        "- **`str.rstrip()`**  \n",
        "  Removes trailing whitespace or specified characters from the right end of a string.\n",
        "\n",
        "- **`str.lstrip()`**  \n",
        "  Removes leading whitespace or specified characters from the left end of a string.\n",
        "\n",
        "- **`str.strip()`**  \n",
        "  Removes both leading and trailing whitespace or specified characters from both ends of a string.\n"
      ],
      "metadata": {
        "id": "padKktQUKi1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following texts with an extra space at either side:"
      ],
      "metadata": {
        "id": "MvmCtsXmLD_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endspace = 'These pretzels are making me thirsty '"
      ],
      "metadata": {
        "id": "3cyQB4ozLC64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startspace = ' These pretzels are making me thirsty'"
      ],
      "metadata": {
        "id": "VoKauXFHLJXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply the `strip()` functions to the entire string before tokenization. This is a quick and effective way to clean up an entire string/text before doing something with it:"
      ],
      "metadata": {
        "id": "Xum7cKoULPft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endspace.rstrip()"
      ],
      "metadata": {
        "id": "h-rRO0dLLSpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startspace.lstrip()"
      ],
      "metadata": {
        "id": "0OxC8tPrLWBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endspace.strip()"
      ],
      "metadata": {
        "id": "U3kIOYTALX4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startspace.strip()"
      ],
      "metadata": {
        "id": "-Kucn8MkLY8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords**\n",
        "\n",
        "Another form of preprocessing is to remove so-called stopwords. Stopwords are frequently occuring **function** words, such as determiners & articles (e.g., *the*, *an*), prepositions (*over*, *in*, *at*), and so on. Contrast these words with content words, such as nouns (e.g., *dog*), verbs (e.g., *run*), and adjectives (e.g., *quick*), and you should begin to see the difference.\n",
        "\n",
        "The logic of removing stopwords is driven by an assumption that stopwords generally contribute very little to the meaning of a text. Stopwords are also not good for distinguishing among texts, because they are so common and used in every text.\n",
        "\n",
        "However, as NLP algorithms have advanced recently, removing stopwords can something be counterproductive. But before we consider why, let's first look into how we could remove stopwords from a text, and the effects this has on the text. The NLTK module has a list of stopwords built-in, run the cell below to see it.\n",
        "\n"
      ],
      "metadata": {
        "id": "jUfuqEMg8gXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in and inspect the stopwords resource\n",
        "import nltk\n",
        "nltk.download(['stopwords', 'punkt_tab'])"
      ],
      "metadata": {
        "id": "tl-OmYSz9aC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the entire stopwords resource\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# loop through all the the English stopwords\n",
        "[word for word in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "aKH4FEh89raX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look through the list above - you can see that there are a lot of words and pieces of words identified as stop words. You can use this list as a check to remove stopwords via a list comprehension.\n",
        "\n",
        "To do so, we can include a conditional test that retains words only if they are *not* a stopword. Let's observe with a sample sentence:"
      ],
      "metadata": {
        "id": "IauzzI1o-Cev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bring in a quote to use:\n",
        "hitchiker = \"\"\"Far Out in the uncharted backwaters of the unfashionable end\n",
        "of the Western Spiral arm of the galaxy lies a small unregarded yellow sun\"\"\""
      ],
      "metadata": {
        "id": "FxN4GNeu_GlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following list comprehension, I test whether the lowercased version of each word is in the stopwords list. Note how this does not transform the original token, but instead uses the lowercased version of the token for the condition test. This means I can have the best of both words - make decision based on a normalised version of the token while retaining the raw version of the token:"
      ],
      "metadata": {
        "id": "45Z0Yt3LHizg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove any token for which the lowercased version is in the NLTK stopwords resource\n",
        "[token for token in nltk.word_tokenize(hitchiker) if token.lower() not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "1CfCCp4LF_qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What effect has removing stopwords had on the text? Can you still understand it?\n",
        "\n",
        "Let's try this out on a longer text:"
      ],
      "metadata": {
        "id": "QGUf3Ax-ICz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/refs/heads/main/sample-texts/marine_biologist.txt'\n",
        "mb = open('marine_biologist.txt').read()"
      ],
      "metadata": {
        "id": "Ig4xaDQNIQKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get an idea of the text - I'll use `nltk.sent_tokenize()` on the text and print out some sentences from near the end:"
      ],
      "metadata": {
        "id": "9D-Jf4CoIlqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first ten sentences of the text\n",
        "for sent in nltk.sent_tokenize(mb)[350:373]:\n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "f8SoYWRrIWpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see what happens when we remove stopwords. Note that I use a combination of `' '.join()` and `nltk.word_tokenize()` within the list comprehension to split each sentence and glue them back together during the list comprehension.\n",
        "\n",
        "\n",
        "What do you think of the output? Can you still understand the 'gist' of the texts?\n"
      ],
      "metadata": {
        "id": "XZ8IyrYRJRyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in nltk.sent_tokenize(mb)[350:373]:\n",
        "  print(' '.join([token for token in nltk.word_tokenize(sent) if token.lower() not in stopwords.words('english')]))"
      ],
      "metadata": {
        "id": "ffFCx8QLJTiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logic behind removing \"stopwords\" is that you could do this for *any* set of words that you want removed from a text. For example, what if I wanted to remove the names of the characters in the script? I could easily define my own list of stopwords and use those in the test:"
      ],
      "metadata": {
        "id": "opCCnZWXJ1Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of target names to remove:\n",
        "stopnames = ['ELAINE', 'JERRY', 'GEORGE', 'KRAMER']"
      ],
      "metadata": {
        "id": "VoFZfnLrKBGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove any the names\n",
        "for sent in nltk.sent_tokenize(mb)[350:373]:\n",
        "  print(' '.join([token for token in nltk.word_tokenize(sent) if token not in stopnames]))"
      ],
      "metadata": {
        "id": "NT3YtNANKH1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More punctuation removal\n",
        "\n",
        "One way we have explored to remove punctuation is to define a string of target punctuation marks and then remove anything in that string using a conditional test. This works well enough if we define all of the punctuation we are interested in removing, although that can be annoying! One nice thing, in English at least, is the presence of the `string` class in Python. This is a stored set of information about English string data you can easily access for convenience. First import `string`\n"
      ],
      "metadata": {
        "id": "HcCU_IhcmDFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "XKx4xDDkNF-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there isn't much here, but the methods `punctuation` and various letters might be useful! We can access these using `string.X`:"
      ],
      "metadata": {
        "id": "vSs6vebnNWKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir(string)"
      ],
      "metadata": {
        "id": "uE6RPUW7NVXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at all the uppercase letters\n",
        "string.ascii_uppercase"
      ],
      "metadata": {
        "id": "_utdJdeuNbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all letters\n",
        "string.ascii_letters"
      ],
      "metadata": {
        "id": "94ElUqzJNlVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all punctuation\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "sDn_LCZqNo72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't confuse the `string` class with the `str` type in Python, they are not the same thing. The `string` class is really just a helper object for you to quickly access letters and punctuation marks, which you could use as needed."
      ],
      "metadata": {
        "id": "myY2cMSENrK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using **`.isalpha()` and `.alnum()`**\n",
        "\n",
        "There is another way to remove punctuation, or but differently, to retain characters that are only alphabetic or alphanumeric. You can do so using the built-in string methods `.isalpha()` and `.isalnum()`. Read the explanation for each one - note how tkhey both refer to a check on the **entire string**.  "
      ],
      "metadata": {
        "id": "gOW3Bu2eOCsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(str.isalpha)"
      ],
      "metadata": {
        "id": "Jtj75YSSOSX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(str.isalnum)"
      ],
      "metadata": {
        "id": "QGqbRkODOXg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So a string in which *any* character violates the test will return `False` for the entire string:"
      ],
      "metadata": {
        "id": "Ii0ElQ9uOo7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# False because the ! is not alphabetic\n",
        "'hi!'.isalpha()"
      ],
      "metadata": {
        "id": "kha_kMNDOlZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# False because the 1 is not alphabetic\n",
        "'hi1'.isalpha()"
      ],
      "metadata": {
        "id": "b8CTOqswOstY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True because 'hi' is alphabetic and '1' is numeric\n",
        "'hi1'.isalnum()"
      ],
      "metadata": {
        "id": "SJaYU0N-O0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this method, we can drop any string that fails this test. When combined with a function such as `nltk.word_tokenize()`, we can run this test on the entire token. Why? Because `nltk.word_tokenize()` will separate out the punctuation into its own separate token!"
      ],
      "metadata": {
        "id": "qK_HSG7YPNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target string\n",
        "jp = 'Life, uh, finds a way.'"
      ],
      "metadata": {
        "id": "YqPYp-U9L5Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check tokenization - the comma and full stop are their own tokens\n",
        "nltk.word_tokenize(jp)"
      ],
      "metadata": {
        "id": "13MIyrr4Padj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `.isalnum()` to \"clean\" out the punctuation from the list of tokens:"
      ],
      "metadata": {
        "id": "MBeFdLhcPgo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[token for token in nltk.word_tokenize(jp) if token.isalnum()]"
      ],
      "metadata": {
        "id": "QtujnhEzL_Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **a simple regex pattern to remove all punctuation**\n",
        "\n",
        "Finally, here is another method for removing punctuation in a text that can be applied to an entire string before tokenization. This method is nice because it does not use a loop and thus does not require any joining at the end. The downside is that it is slightly more complex to understand, because it uses a somewhat opaque regular expression pattern.\n",
        "\n",
        "Using this slightly more complex regular expression pattern, we can remove all punctuation in a string. The pattern is:\n",
        "\n",
        "```\n",
        "[^\\w\\s]\n",
        "```\n",
        "\n",
        "This pattern contains two token representations:\n",
        "\n",
        "- `\\w` matches any word character (equivalent to `[a-zA-Z0-9_]`)\n",
        "- `\\s` matches any whitespace character (equivalent to `[\\r\\n\\t\\f\\v  ]`)\n",
        "\n",
        "The `^` is a metacharacter which negates the pattern, saying anything *but* these things. Because the `^` is inside the square brackets `[]`, it says anything **but** what is inside these brackets.\n",
        "\n",
        "So, with a bit of reverse logic, the pattern below means replace anything that is *not* a word character or whitespace character with nothing, effectively cleaning out punctuation:\n",
        "\n",
        "```\n",
        "re.sub(r'[^\\w\\s]', '', text)\n",
        "```\n",
        "\n",
        "It's okay if this is a bit over your head, but remember this pattern and come back to copy this cell when you need to remove punctuation in the future! Also keep in mind there might be instances where you want to more carefully control the removal of specific characters, so be careful!"
      ],
      "metadata": {
        "id": "nUq4kwNCMtg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "bZM8fE1mMz4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an easy way to remove punctuation from a string\n",
        "re.sub(r'[^\\w\\s]', '', jp)"
      ],
      "metadata": {
        "id": "NlBtsY5TnOm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}