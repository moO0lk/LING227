{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moO0lk/LING227/blob/main/13_creating_NLTK_corpora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating your own NLTK corpus\n",
        "\n",
        "Regardless of how you get your data into Colab, you can use the NLTK library to make your own version of the NLTK corpora.\n",
        "\n",
        "There are two ways to do this, one is to read in a bunch of texts as one single corpus. To do this, we use the `PlaintextCorpusReader` class from NLTK.\n",
        "\n",
        "In order to use it, we need three things:\n",
        "\n",
        "1. some files,\n",
        "2. a filepath which leads to files, and\n",
        "3. the names of the files.\n",
        "\n",
        "Let's start with some data! The following cell will download several scripts from an American television show *Seinfeld*. The zip file will be downloaded into the notebook environment.  \n",
        "\n"
      ],
      "metadata": {
        "id": "OvJtd0u4RMSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://github.com/scskalicky/LING-226-vuw/raw/main/other-data/seinfeld.zip'"
      ],
      "metadata": {
        "id": "-So4had-dNc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to extract or unzip the contents of zip file to our notebook - the following cell does so using the `!unzip` command. There's `-d` flag unzips into a directory to make working with the data easier. So the command `!unzip` will be run on the `seinfeld.zip` folder and the results will be a new directory `-d` named `seinfeld`\n"
      ],
      "metadata": {
        "id": "HTN0PqVKdj-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"seinfeld.zip\" -d \"seinfeld\""
      ],
      "metadata": {
        "id": "vlEkGsfvdpqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_root = '/content/drive/MyDrive/seinfeld'"
      ],
      "metadata": {
        "id": "G2MvWvdrNk2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll load in the corpus reader class from NLTK"
      ],
      "metadata": {
        "id": "yZaiWKvhTCm2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIK6K6ZLU8mn"
      },
      "source": [
        "# import the module to read in plain text\n",
        "from nltk.corpus import PlaintextCorpusReader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as some other required NLTK resources"
      ],
      "metadata": {
        "id": "RJewgL9b3PzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the NLTK library\n",
        "import nltk\n",
        "\n",
        "# download resources\n",
        "nltk_resources = ['gutenberg', 'punkt_tab', 'brown', 'state_union']\n",
        "\n",
        "nltk.download(nltk_resources)"
      ],
      "metadata": {
        "id": "35KrMkoA3RbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to create a new variable from the `PaintextCorpusReader` which will become our corpus!\n",
        "\n",
        "The first variable is `root` which allows us to specify where the corpus lives. In this case, the corpus is within the `seinfeld` folder.\n",
        "\n",
        "The second argument is `fileids` which asks for the list of files. The files in the seinfeld folder are:\n",
        "\n",
        "```\n",
        "THE BOYFRIEND PT 1_cleaned.txt\n",
        "THE BOYFRIEND PT 2_cleaned.txt\n",
        "THE CHINESE RESTAURANT_cleaned.txt\n",
        "THE DEALERSHIP_cleaned.txt\n",
        "THE DOODLE_cleaned.txt\n",
        "THE ENGLISH PATIENT_cleaned.txt\n",
        "THE FACE PAINTER_cleaned.txt\n",
        "THE GOOD SAMARITAN_cleaned.txt\n",
        "THE JUNIOR MINT_cleaned.txt\n",
        "THE LITTLE KICKS_cleaned.txt\n",
        "THE MARINE BIOLOGIST_cleaned.txt\n",
        "THE PARKING GARAGE_cleaned.txt\n",
        "THE PARKING SPACE_cleaned.txt\n",
        "THE PEZ DISPENSER_cleaned.txt\n",
        "```\n",
        "\n",
        "Let's try it out on a single file to start.\n"
      ],
      "metadata": {
        "id": "ockSVnpbTIEa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr-m9X4CNt38"
      },
      "source": [
        "# read in my text (i've passed the name in a list, so I could include more than one text if I need to later)\n",
        "marine_biologist_corpus = PlaintextCorpusReader(root = 'seinfeld/', fileids = ['THE MARINE BIOLOGIST_cleaned.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've created a corpus (even if it is just one text), we can use the built-in NLTK corpus functions."
      ],
      "metadata": {
        "id": "nN5IxJtRUrB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The raw version should be just the string\n",
        "marine_biologist_corpus.raw()[15041:15135]"
      ],
      "metadata": {
        "id": "GgXPlelEUoyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also get sentences\n",
        "marine_biologist_corpus.sents()"
      ],
      "metadata": {
        "id": "GVJBVsyXVCJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you remember from the first part of NLTK, they were using functions like `.concordance()` on the built-in data. We can do the same with our data, but we need to wrap the tokenized words in an `nltk` function called `Text()`."
      ],
      "metadata": {
        "id": "tk2nchqGV4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a special Text version of the corpus\n",
        "from nltk.text import Text\n",
        "mb_txt = Text(marine_biologist_corpus.words())"
      ],
      "metadata": {
        "id": "CKAHoHt-WCE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can look for concordance lines\n",
        "mb_txt.concordance('GEORGE')"
      ],
      "metadata": {
        "id": "mnAXtzDOWv5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb_txt.concordance('whale')"
      ],
      "metadata": {
        "id": "b5OrtsAGXISG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in multiple texts to make a corpus\n",
        "\n",
        "A corpus of a single text is not very interesting. Let's update our `PlaintextCorpusReader` to include all of the texts in our Seinfeld folder. But, it sure would be annoying having to type all of the filenames one-by-one. Fortunately, there are several ways around this.\n",
        "\n",
        "One is to use the [`glob` library](https://docs.python.org/3/library/glob.html) to quickly all of the filenames in a directory. The `glob` function makes it easy to save all of the filenames from a directory into a variable.  "
      ],
      "metadata": {
        "id": "FAX-0zaZXmbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the function which is the same name as the module\n",
        "from glob import glob\n",
        "\n",
        "# the * indicates you want everything from the folder.\n",
        "# we can use more intelligent ways to select only certain files, we'll see this later with regex\n",
        "filenames = glob('/content/seinfeld/*')\n",
        "\n",
        "filenames"
      ],
      "metadata": {
        "id": "rkyB672wYomd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing this gives us the entire filepath which doesn't really hurt us but also is kind of annoying. We could easily remove this using slicing. Because the part that we want to remove is always the same (i.e., the `/content/seinfeld/'` part), we could just slice that part off from each filename. All we need to know is where to start the slice"
      ],
      "metadata": {
        "id": "MR_hVkmvZ6hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting at 18 gives us the episode name only.\n",
        "filenames[1][18:]"
      ],
      "metadata": {
        "id": "5LFBJvu8ZA6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's write a list comprehension which removes the start of each filename\n",
        "filenames_short = [name[18:] for name in filenames]\n",
        "\n",
        "# voila!\n",
        "filenames_short"
      ],
      "metadata": {
        "id": "qPazBRCOaQt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can just pass `filenames_short` to the `PlaintextCorpusReader` function and make a larger corpus. I tested it and it will also work without cleaning the filepath we get from `glob`, but this is nice because we remove the clutter."
      ],
      "metadata": {
        "id": "P3ejP2OUaewQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make our seinfeld corpus\n",
        "seinfeld_corpus = PlaintextCorpusReader(root = 'seinfeld/', fileids = filenames_short)"
      ],
      "metadata": {
        "id": "_mHOeHTwZmuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use the fileids function to see the texts in here\n",
        "seinfeld_corpus.fileids()"
      ],
      "metadata": {
        "id": "L-HcLKA_ZzA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the ten most common words in our corpus?\n",
        "from nltk import FreqDist\n",
        "FreqDist(seinfeld_corpus.words()).most_common(10)"
      ],
      "metadata": {
        "id": "dDVeMgpJayXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and I can search for concordances, neat!\n",
        "Text(seinfeld_corpus.words()).concordance('apartment')"
      ],
      "metadata": {
        "id": "yIM7mCSUbk9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading data from your Google Drive folders\n",
        "\n",
        "If you have data within your Google Drive you want to use, you just need to amend the above code to point at those folders. This means your corpus roots will be something like `content/drive/MyDrive/...`\n",
        "\n",
        "You'll also need to mount the drive!"
      ],
      "metadata": {
        "id": "E0gxHhebfxPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Your Own Categorized Corpus\n",
        "\n",
        "The next type of corpus you can make is a categorised corpus, which will allow you to compare groups of files within your corpus.\n",
        "\n",
        "In order to do so, we need some text files, and we also need a way to indicate what genre/category we would like those files to belong to. The NLTK authors do this by extracting information (i.e., metadata) from the filenames.\n",
        "\n",
        "As an example, let's use some data from a [paper I published in 2015.](https://europeanjournalofhumour.org/index.php/ejhr/article/view/68)\n",
        "\n",
        "In this paper, I analysed the linguistic properties of product reviews written for the American retail website Amazon.com. I was interested in two types of reviews: legitimate review and satirical/funny reviews.\n",
        "\n",
        "The data lives here: [Amazon Data](https://github.com/scskalicky/LING-226-vuw/blob/main/other-data/amazon%20reviews.zip)\n",
        "\n",
        "We can again use `!wget` and `!unzip` to load in a zip file and save to the notebook environment. Run the code cell below to download and unzip the data into the notebook:\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw11e4z6vM9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data\n",
        "!wget 'https://github.com/scskalicky/LING-226-vuw/raw/main/other-data/amazon%20reviews.zip'"
      ],
      "metadata": {
        "id": "wqzEzPqP8oDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now unzip the data into the environment:\n"
      ],
      "metadata": {
        "id": "OcbFZe209YEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"amazon reviews.zip\" -d \"amazon reviews\""
      ],
      "metadata": {
        "id": "hsfYBZLa9lwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the folder are 375 normal reviews and 375 satirical reviews.\n",
        "\n",
        "The name of each file looks like this:\n",
        "```\n",
        "001-5-satire.txt\n",
        "002-2-normal.txt\n",
        "```\n",
        "\n",
        "The first three numbers are the ID number, ranging from 1 - 375. The second number (between the two `-`) is the star rating of the review, from 1-5. The words `satire` or `normal` indicate whether the review was a normal review or a satirical funny review.\n",
        "\n",
        "We can exploit this information to make categories in our corpus. Just as the authors of NLTK sliced the year from the filename to examine change over time, we can do the same thing with these filenames to get different categories.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JnsujtP29AZ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ct3AZQSNJvJ"
      },
      "source": [
        "# first we will load in the Corpus Reader and define the location of our texts\n",
        "import nltk\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
        "\n",
        "# set the corpus location to point to wherever it is you saved the data\n",
        "# (you may need to mount your Drive to the notebook)\n",
        "corpus_location = '/content/amazon reviews'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to use the filenames as categories, we will use a regular expression (regex) pattern to define a pattern to capture the `normal` or `satire` portions of the filesnames using this pattern:\n",
        "\n",
        "```\n",
        ".*(......).txt\n",
        "```\n",
        "\n",
        "This pattern captures whatever is in the brackets `()`, and says give me the last six characters before `.txt` of my pattern.\n",
        "\n",
        "It corresponds to:\n",
        "\n",
        "```\n",
        "001-5-(satire).txt\n",
        "002-2-(normal).txt\n",
        "```\n",
        "\n",
        "Try it out:"
      ],
      "metadata": {
        "id": "caCO2-GD0H8j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGNNRvuFQlt_"
      },
      "source": [
        "# create a categorised corpus\n",
        "amz_corpus = CategorizedPlaintextCorpusReader(root = corpus_location, fileids = '.*', cat_pattern = '.*(......).txt')\n",
        "\n",
        "# you can check the categories\n",
        "amz_corpus.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYJT4iMSNgAd"
      },
      "source": [
        "# and we still have our fileids\n",
        "amz_corpus.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've made our corpus, we can create CFD tabulations and plots just like the NLTK book did for Brown corpus.\n",
        "\n",
        "Let's compare different words between the satirical and regular reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "RZvxSBNl1U5M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G51ezHQGSBlv"
      },
      "source": [
        "# Create a CFD of the amazon corpus\n",
        "# I am using the same code as the one for Brown with two modifications:\n",
        "# I have replaced \"genre\" with \"review_type\"\n",
        "# I lowercase the words in the corpus\n",
        "amz_cfd = nltk.ConditionalFreqDist(\n",
        "    (review_type, word)\n",
        "    for review_type in amz_corpus.categories()\n",
        "    for word in [w.lower() for w in amz_corpus.words(categories = review_type)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's ask for some specific words\n",
        "pronouns = ['i', 'me', 'you', 'my', 'yours', 'them']\n",
        "\n",
        "# then tabulate them\n",
        "amz_cfd.tabulate(conditions = ['normal', 'satire'], samples = pronouns, cumulative = True)"
      ],
      "metadata": {
        "id": "jOBVORFf4GzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The raw counts are interesting but not really helpful without being normalised somehow. Let's plot the data using the `percents = True` argument to convert the counts into percents of the entire corpus. These allow us to make more fair comparisons."
      ],
      "metadata": {
        "id": "ZI8shHuIg7tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also plot this.\n",
        "amz_cfd.plot(conditions = ['normal', 'satire'], samples = pronouns, cumulative = False, percents = True)"
      ],
      "metadata": {
        "id": "lt1Xn0c75nvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you see in the plot? Does any one category have any more/less of a particular word?\n",
        "\n",
        "We can try this out using any number of target words:"
      ],
      "metadata": {
        "id": "L5ZZEiBBhK6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what about some other words?\n",
        "emotions = ['good', 'bad', 'happy', 'sad', 'love', 'sweet', 'hurt', 'ugly', 'nasty']\n",
        "amz_cfd.tabulate(conditions = ['normal', 'satire'], samples = emotions, cumulative = True)"
      ],
      "metadata": {
        "id": "2tUP5NRG4ygk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amz_cfd.plot(conditions = ['normal', 'satire'], samples = emotions, cumulative = False, percents = True)"
      ],
      "metadata": {
        "id": "Im9eh2nPhW-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also wrap individual files from our corpus in `Text` so that we can look for concordances"
      ],
      "metadata": {
        "id": "XVzkqcKP5wBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the whole set of words to look at all concordances\n",
        "nltk.text.Text(amz_corpus.words()).concordance('terrible')"
      ],
      "metadata": {
        "id": "QEn5PLCS70xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also look at concordances for just one category to compare them\n",
        "# the word \"banana\" is strongly associated with the satire corpus\n",
        "nltk.text.Text(amz_corpus.words(categories = 'satire')).concordance('banana')"
      ],
      "metadata": {
        "id": "VN8Jr7hk8Un8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but only occurs once in the non-satire corpus.\n",
        "nltk.text.Text(amz_corpus.words(categories = 'normal')).concordance('banana')"
      ],
      "metadata": {
        "id": "Hgr_RbXm8epr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A more complex method to create categories\n",
        "\n",
        "> You don't really need to worry about this unless you think you need a more complex corpus / category structure.\n",
        "\n",
        "You might not want to use part of the file names to create a categorized corpus reader in NLTK (in fact, doing so requires you to have a specific naming convention and potentially use more complicated regex syntax to parse the names). You might have a bunch of files, maybe even in different folders. Rather than trying to find a consistent way to put metadata into the filename, you can instead supply a file that provides the metadata for your corpus.\n",
        "\n",
        "\n",
        "\n",
        "This file should contain a list of all the paths for your texts, followed by the category you want to assign to those texts.\n",
        "\n",
        "For example the text file could look like this:\n",
        "\n",
        "```\n",
        "folder1/file1.txt categoryA\n",
        "folder1/file2.txt categoryA\n",
        "folder2/file3.txt categoryB\n",
        "\n",
        "```\n",
        "\n",
        "You can save this as a `.txt` file named something like `categories.txt`.\n",
        "\n",
        "Then, you pass the location of this file to the `cat_file` argument of the `CategorizedPlaintextCorpusReader` class.\n",
        "\n",
        "Here is an example.\n",
        "\n",
        "First download a sample corpus.\n"
      ],
      "metadata": {
        "id": "EXlLq-15jFUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://github.com/scskalicky/LING-226-vuw/raw/main/other-data/corpus.zip'"
      ],
      "metadata": {
        "id": "zpmerFnikhq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the corpus into the content folder of the notebook environment\n",
        "!unzip 'corpus.zip' -d '/content'"
      ],
      "metadata": {
        "id": "0KUlCoxXkq2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will also need your file which maps the categories, ideally outside the folder that your corpus is in. I download this one to the main notebook environment."
      ],
      "metadata": {
        "id": "xXxPzP-ubnCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://github.com/scskalicky/LING-226-vuw/raw/main/other-data/categories.txt'"
      ],
      "metadata": {
        "id": "lnQ_H-KXa-_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the contents of the categories file: You can see it lists the file name of each file, followed by a space and the category for that file.\n",
        "\n"
      ],
      "metadata": {
        "id": "uffmFi4xj_Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(open('categories.txt').read())"
      ],
      "metadata": {
        "id": "NRnHgWXvj7dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now load in the categorized corpus class from NLTK\n"
      ],
      "metadata": {
        "id": "4KZyeVVjkDND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "metadata": {
        "id": "9a0hj6jiW5hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declare your corpus and give the corpus reader three pieces of information:\n",
        "\n",
        "1. The root folder for your corpus. Since this is in Colab, the first root will be `content`. The folder we uploaded is called `corpus`, so our root is `/content/corpus/`\n",
        "\n",
        "2. The names of the files, which will be all the `.txt` files in the folders\n",
        "\n",
        "3. The name of the `.txt` file containing the mapping of file names to categories. The `..` is there to tell the function that the `categories.txt` file is in one directory above the corpus folder. Otherwise, the function will not find the file!"
      ],
      "metadata": {
        "id": "qeStMnxKkFZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = CategorizedPlaintextCorpusReader(root = '/content/corpus', fileids = '.*.txt', cat_file = '../categories.txt' )"
      ],
      "metadata": {
        "id": "EDooRuB1Xyv4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the categories of the corpus:"
      ],
      "metadata": {
        "id": "L4YuNzu_cBmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.categories()"
      ],
      "metadata": {
        "id": "E44KFB1NX8P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as the filenames:"
      ],
      "metadata": {
        "id": "inp6Qvi0cEcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.fileids()"
      ],
      "metadata": {
        "id": "3zTG8flfn_Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the `categories` argument to look at files in any one category:"
      ],
      "metadata": {
        "id": "yy4HDKZbcG1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for category in corpus.categories():\n",
        "  for file in corpus.fileids(categories = category):\n",
        "    print(corpus.raw(file), '\\t', category)"
      ],
      "metadata": {
        "id": "-NemQdt6eEoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is probably the best method to use if you have a complex set of files and folders that you want to be able to place into different categories without relying on filenames."
      ],
      "metadata": {
        "id": "66PwQnurceCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wrap Up**\n",
        "\n",
        "Being able to create your own corpus and make a comparison across categories in your corpus within NLTK will provide you with a way to compare texts across categories.\n"
      ],
      "metadata": {
        "id": "TFlvDWpK9Ad-"
      }
    }
  ]
}