{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moO0lk/LING227/blob/main/09_word_frequencies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Relevant readings\n",
        "\n",
        "[NLTK Book, Chapter 1, Section 3.1 and 3.2](https://www.nltk.org/book/ch01.html#frequency_distribution_index_term)"
      ],
      "metadata": {
        "id": "BKg_42kmeauu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxGe75vjJ1U"
      },
      "source": [
        "# **Word Frequencies**\n",
        "\n",
        "In this notebook we will learn how to explore a fundamental numerical distribution of language: the relative frequencies of words.  Word frequency is a very interesting property of language because it correlates with other constructs, such as word length (shorter words are more frequent) and word difficulty (more complex words are less frequent).\n",
        "\n",
        "Another interesting things about frequency is a phenomena called Zipf's law, which states that the most frequent word occurs at least twice as much as the second most frequent word, and this this relationship persists. You can read a [reddit post about it here](https://www.reddit.com/r/linguistics/comments/830nf5/zipfs_law_was_so_cool_that_i_performed_and/), or at least look at the person's graph they made explaining the phenomenon:\n",
        "\n",
        "\n",
        "<img src = https://www.etymologynerd.com/uploads/1/5/8/8/15888322/website.png height = \"400\">\n",
        "\n",
        "\n",
        "Moreover, counting the frequency in which words occur with *other* words has proven very insightful for linguistics and NLP. The most basic insight is that words tend to co-occur with other specific words in predictable ways. Corpus linguists call these pairs of words `collocations`, and define them using a variety of different statistical measures. Finding these larger collocational patterns has given strength to functional lingusitic theories of grammar such as construction grammar, which argue that both meaning and syntax determine the way a word is used in language (contrast this with a purely structural approach, which argues grammar rules exist independently of meaning).\n",
        "\n",
        "Word co-occurence statistics are also used to create co-occurence distributions and vector spaces - these are what large-scale NLP algorithms and artificial intelligence applications rely on for word predictions in both processing and production (more on that later!).\n",
        "\n",
        "The second half of NLTK Chapter 1 begins to introduce these important concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ALUFQLq0lw"
      },
      "source": [
        "## **Frequency distributions**\n",
        "\n",
        "\n",
        "The simplest form of a frequency distribution is a count of how many times each word appears in a text. It's worth pausing for a moment and considering how you might construct your own frequency distribution with Python and some text â€” what might be the steps for doing so? Here is one general approach you could take:\n",
        "\n",
        "1. You start a loop over the words in a text (ideally already tokenized)\n",
        "2. At the first word, you note down the word and store it in a separate data container, alongside a value representing its frequency (starting with 1)\n",
        "3. You then move to the next word and check if the next word already exists in your data container,\n",
        "      - if it does already exist, you increase its count by 1\n",
        "      - If it does not exist, you add it to the data container and set an initial count of 1\n",
        "\n",
        "Here is what that might look like using pseudocode:\n",
        "\n",
        "```\n",
        "output_container = []\n",
        "\n",
        "for word in list_of_tokens:\n",
        "  if word in output_container\n",
        "    increase count of word + 1\n",
        "  else\n",
        "    add word to output_container\n",
        "    increase count of word + 1\n",
        "```\n",
        "\n",
        "Now, what kind of data container would make sense for this? A `list` might be able to work, but this would require some careful slicing and indexing and might become a pain. There is another data container better designed for this known as a `dictionary`. We will learn how to create dictionaries in a later lesson. But for now, we can rely on a built-in NLTK function named `FreqDist()`, which creates a dictionary of `value:frequency` pairs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using `nltk.FreqDist()`**\n",
        "\n",
        "We can pass a sequence (e.g., a string, a list, etc) to the `nltk.FreqDist()` function and it will count the number of times different values in the sequence occur. For example, we can count the frequency of letters in a word, or the frequency of words in a sentence, and so on.\n",
        "\n",
        "Let's look at an example. To prepare, I first import the `FreqDist()` function, then create a text, split the text into tokens using `.split()`."
      ],
      "metadata": {
        "id": "msG5As1oZrFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first import the FreqDist function from nltk\n",
        "from nltk import FreqDist"
      ],
      "metadata": {
        "id": "_sfTYTOfaPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing many characters\n",
        "turtles = \"\"\"teenage mutant ninja turtles\n",
        "            teenage mutant ninja turtles\n",
        "            teenage mutant ninja turtles\n",
        "            heroes in a halfshell, turtle power\"\"\""
      ],
      "metadata": {
        "id": "XKJQ6WyGVCkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the string into tokens/words using .split()\n",
        "turtles_tokens = turtles.split()\n",
        "\n",
        "turtles_tokens"
      ],
      "metadata": {
        "id": "oNLex3YBVGeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a list of tokens, we pass that list of tokens to the `FreqDist()` function, giving it the variable name `turtle_fdist`:"
      ],
      "metadata": {
        "id": "hZluEMKIVS6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the frequency distribution to a variable\n",
        "turtle_fdist = FreqDist(turtles_tokens)"
      ],
      "metadata": {
        "id": "K6u2loviVJ5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the results:"
      ],
      "metadata": {
        "id": "SRRMAsg8Vjwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the frequency distribution\n",
        "turtle_fdist"
      ],
      "metadata": {
        "id": "cZKOycL0VkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting frequency distribution is a Python data object called a `dictionary` which stores `key:value` pairs. In this case, our keys are the words, and the values are the frequencies.\n",
        "\n",
        "We can query a dictionary for specific `key:value` pairs using the following syntax:\n",
        "\n",
        "> `dictionary['key']`\n",
        "\n",
        "This should look familiar, because it is similiar to how one can index characters in strings (e.g., `turtles[1]`) or index words in lists (e.g., `['one', 'two'][0]`)\n",
        "\n",
        "So, we can ask the Frequency Distribution to give us the frequency for a target word. For example, the words `turtles`:"
      ],
      "metadata": {
        "id": "016Nks4Aadz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtles?\"\n",
        "turtle_fdist['turtles']"
      ],
      "metadata": {
        "id": "Gv8qr37a3uw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtle?\"\n",
        "turtle_fdist['turtle']"
      ],
      "metadata": {
        "id": "pCqS9l6l39yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we ask for a word that is *not* in the distribution, we receive an answer of 0, which makes sense."
      ],
      "metadata": {
        "id": "vv9SY4HHV6Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what happens if we ask for a word not in the dictionary?\n",
        "# the NLTK FreqDist gives us a 0 rather than an error, which is handy!\n",
        "turtle_fdist['shredder']"
      ],
      "metadata": {
        "id": "f-f31JQybyo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`FreqDist() functions`**\n",
        "\n",
        "A `FreqDist()` object comes with some built-in helper functions. For example, we can ask for the most frequent terms from a frequency distribution using the `.most_common()` method. We can specify the number of top results we want by putting a number in the brackets `()` used by `.most_common()`. The code below has a `3` in the brackets, so the function will return the top-three most frequent words in the frequency distribution."
      ],
      "metadata": {
        "id": "kMSZrRTE4Csl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what are the top 3 most frequent words?\n",
        "turtle_fdist.most_common(3)"
      ],
      "metadata": {
        "id": "swc9GHLj4GUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also ask for so-called `hapaxes` - words that occur only one time in the distribution:"
      ],
      "metadata": {
        "id": "0PiXQ4rWWPu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtle_fdist.hapaxes()"
      ],
      "metadata": {
        "id": "w00RahlfWS0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can query the total number of word entries in the distribution using `FreqDist.N()`. In other words, this is the total number of **types** in the text (not tokens!)."
      ],
      "metadata": {
        "id": "NDavf_qNWZ2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtle_fdist.N()"
      ],
      "metadata": {
        "id": "Eycg9wz9WgCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-tuning a search with frequency**\n",
        "\n",
        "Lets calculate word frequencies for a larger, more interesting data set. Create a frequency distribution of the webchat corpus included with `nltk` which is `text5` using `FreqDist()`. We need to import `nltk` and download the `book` resource:"
      ],
      "metadata": {
        "id": "TgOteauK4UU1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjC8cj2mq8eu"
      },
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download(['book', 'punkt_tab'])\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdwfZBrdXw_-"
      },
      "source": [
        "# Now create a FreqDist of the webchat text\n",
        "webchat_fdist = FreqDist(text5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWWHK6mCZurG"
      },
      "source": [
        "What are the 50 most common words in the webchat corpus? Examine the output - what do you see? Are there items in the output you did or did not expect? What do you think is happening?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Rj9MVHZuS4"
      },
      "source": [
        "webchat_fdist.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Q5rxIDaEp-"
      },
      "source": [
        "Let's now look at how people use the phrase \"lol\" - both the individual frequency and the overall percentage of \"lol\" in the corpus.\n",
        "\n",
        "What do you think about the results? 1.5% might seem low, but is actually a rather strong result considering how many possible words *could* be in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxsvV-_OaGbB"
      },
      "source": [
        "# index the value by using the key (in this case, the word we want to check)\n",
        "webchat_fdist['lol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZeygy7aM8z"
      },
      "source": [
        "# divide the frequency of 'lol' by the total length of the corpus, then multiply by 100\n",
        "(webchat_fdist['lol']/len(text5))*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZpSoCv8cwOt"
      },
      "source": [
        "We can now include word frequency as an additional condition when looking for certain words. Do you recall how list comprehensions and conditional for loops worked? For example, if we wanted to ask for all words which are three letters long:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ibJEPgeJLq",
        "collapsed": true
      },
      "source": [
        "# this says: give me every word in text5 but only if the length of the word is equal to 3\n",
        "[w for w in text5 if len(w) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurxcAVLeQaW"
      },
      "source": [
        "The output is not very readable, is it? We are getting every single token which is 3 characters long, including repetitions. We can reduce this firstly by wrapping the list comprehension in `set()` so that we get a list of types, rather than tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcEV-c0FeEGI",
        "collapsed": true
      },
      "source": [
        "# add set()\n",
        "set([w for w in text5 if len(w) == 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUvNzOCYesJI"
      },
      "source": [
        "If you look through that output, you can see that there are a lot of things that look like codes or other non-word stuff, usually in UPPERCASE. We can try removing those using `.islower()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZvWxzUpdNh0",
        "collapsed": true
      },
      "source": [
        "# all tokens which are 3 letters long and all characters are lowercase\n",
        "# give me the set of all words in text5 if the word is 3 characters long and each character is in lower case\n",
        "set([w for w in text5 if len(w) == 3 and w.islower()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZ0dP1LejH6"
      },
      "source": [
        "Now it's getting more manageable. It's still quite a long list though. Let's add another condition - asking for the same output as the previous code, but this time setting a minimum frequency. We can embed a `FreqDist` as part of the condition.  \n",
        "\n",
        "Here I include a conditional that the word must have a frequency greater than 100 in the frequency distribution. I also adjust our length so that we let both 3 and 4 letter words appear.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxfbvIxRd4De",
        "collapsed": true
      },
      "source": [
        "# adding minimum frequency, allow for both 3 and 4 letter words (how else could you write that conditional?)\n",
        "\n",
        "# give me the set of all words in text 5 if the word is 3 or 4 letters long, and is lower case, and occurs over 100 times in the fredist\n",
        "set([w for w in text5 if len(w) <= 4 and len(w) >= 3 and w.islower() and webchat_fdist[w] > 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5o-hh_f62F"
      },
      "source": [
        "What do you see in that output? Any words stand out as representative of a chat corpus? What kinds of words do you think you will find using the same criteria but on a different corpus? The point, which was made in the NLTK book regarding the length of words, is that a single line of code with the right tuning can provide relatively precise insight into the nature of a text and/or corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Your Turn**\n",
        "\n",
        "Spend some time to play around with one of the other built-in texts (`text1` through `text8`) from the NLTK data.\n",
        "\n",
        "Your goal is to try and refine some search patterns to find words which seem to capture the nature of the different texts. For example, you could think about a minimum frequency and minimum or maximum length, such as I have done with `text3` above.\n",
        "\n",
        "You can see what the name of each text is by typing `text1-9` into a cell and running it, for example:"
      ],
      "metadata": {
        "id": "l9PaxO1LkKe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Frequency plots**\n",
        "\n",
        "The NLTK `FreqDist` object also has a `.plot()` method, which allows you to plot the results. Let's explore this with yet another text. We'll use the text *They're Made Out of Meat*. The following cell download the text and loads it for us."
      ],
      "metadata": {
        "id": "54HBn1BBXYsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab allows for terminal functions like `wget`\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/refs/heads/main/sample-texts/tmoom.txt'\n",
        "# open the file as a string into Python\n",
        "tmoom = open('tmoom.txt').read()"
      ],
      "metadata": {
        "id": "TJ859zSchpKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's remove punctuation and lowercase the text. I'll use the regex method.\n"
      ],
      "metadata": {
        "id": "VrJ1rLJNh73W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import re module\n",
        "import re\n",
        "\n",
        "# remove punctuation and lowercase the text\n",
        "tmoom_cleaned = re.sub(r'[^\\w\\s]', '', tmoom.lower())"
      ],
      "metadata": {
        "id": "CRLv7afSij3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# about 780 tokens\n",
        "tmoom_tokens = nltk.word_tokenize(tmoom_cleaned)\n",
        "len(tmoom_tokens)"
      ],
      "metadata": {
        "id": "SzzzjdXTh-mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# of about 300 types\n",
        "tmoom_types = set(tmoom_tokens)\n",
        "len(tmoom_types)"
      ],
      "metadata": {
        "id": "rUt4GzNSiCd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I've cleaned the text up a bit, time to run a frequency distribution on it:"
      ],
      "metadata": {
        "id": "XFcSEa87i9AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom_fdist = nltk.FreqDist(tmoom_tokens)"
      ],
      "metadata": {
        "id": "Y4b17cLni_nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are concerned with frequency, it's interesting to first see how many words only occur one time in the text:"
      ],
      "metadata": {
        "id": "HGQs5b-2jGfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hapaxes shows us words with a frequency of 1\n",
        "tmoom_fdist.hapaxes()"
      ],
      "metadata": {
        "id": "iuYS4rY2jDOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at the top ten most frequency words:"
      ],
      "metadata": {
        "id": "feZEIqc-jTCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom_fdist.most_common(10)"
      ],
      "metadata": {
        "id": "gd5k3IZojO0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use the `.plot()` method to see the distribution. I will add some code which makes the plot larger"
      ],
      "metadata": {
        "id": "LC98CkIkjVfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use this code to make the plot larger\n",
        "import matplotlib.pyplot as plt\n",
        "# define the size of the figure\n",
        "plt.figure(figsize = (20, 10))\n",
        "\n",
        "# then plot the frequency distribution\n",
        "tmoom_fdist.plot()"
      ],
      "metadata": {
        "id": "xhCEVk30jXib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It might be a little bit hard to see still, but do you notice how this plot resembles the Zipfian distribution, like the one we saw at the beginning of the notebook?\n",
        "\n",
        "Do you also see where all the hapaxes are? The words with a frequency of 1 form a flat line on the x-axis.\n",
        "\n",
        "Let's create a second plot which does not include hapaxes. In the cell below, I use a loop over the hapaxes and then delete them from the FreqDist using the `del` command:"
      ],
      "metadata": {
        "id": "bFXzCo98jvhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for hapaxe in tmoom_fdist.hapaxes():\n",
        "  del tmoom_fdist[hapaxe]"
      ],
      "metadata": {
        "id": "opNulOCTkD0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can double check that this has deleted the hapaxes - there are none left."
      ],
      "metadata": {
        "id": "WZ3wVB-RkfmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom_fdist.hapaxes()"
      ],
      "metadata": {
        "id": "KmlnKR4LkfEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the plot again. Look at this! We can clearly see the distribution now, noting the most frequent word(s) in the text and also how frequency takes an absolute nose-dive as we proceed along the x-axis.\n",
        "\n",
        "Consider how else this plot might change if we removed stopwords or if we took another approach to cleaning the punctuation."
      ],
      "metadata": {
        "id": "T58PPm4Jkl1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the size of the figure\n",
        "plt.figure(figsize = (20, 10))\n",
        "\n",
        "# then plot the frequency distribution\n",
        "tmoom_fdist.plot()"
      ],
      "metadata": {
        "id": "dtweallpkmpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A frequency distribution will count anything you give it, which means we can apply this same logic to characters as well as tokens. Let's look at the most frequent characters in the text."
      ],
      "metadata": {
        "id": "XfQrsDCLlzLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom_character_fdist = nltk.FreqDist(tmoom_cleaned)"
      ],
      "metadata": {
        "id": "0CCMUJ6Nl5bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll remove the entries for whitespace and newlines and two numbers so that all is left is the 26 letters of the English alphabet:"
      ],
      "metadata": {
        "id": "p4QL2NTDmQ15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del tmoom_character_fdist[' ']\n",
        "del tmoom_character_fdist['\\n']\n",
        "del tmoom_character_fdist['5']\n",
        "del tmoom_character_fdist['4']"
      ],
      "metadata": {
        "id": "zlRpLzSjmVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the alphabet - what do you think? Does the frequency of these letters make sense?"
      ],
      "metadata": {
        "id": "pMoLWrSKmZkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmoom_character_fdist.plot()"
      ],
      "metadata": {
        "id": "mQQpuoPIl9VB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}