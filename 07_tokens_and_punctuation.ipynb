{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moO0lk/LING227/blob/main/07_tokens_and_punctuation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What! is? a. Token?!**\n",
        "\n",
        "\n",
        "In this notebook we return to the problem of tokenizing a text and defining what counts as a word. So far we've already been doing this with the `.split()` function, which has worked relatively well for us. But, there is one issue, which is that splitting on white space means that sometimes punctuation is included with our words.\n",
        "\n",
        "For example, running `.split()` on the example below will retain commas and exclamation marks as part of the words:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ee8n1A6AlehM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtles = \"\"\"teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            teenage mutant ninja turtles,\n",
        "            heroes in a halfshell, turtle power!\"\"\"\n",
        "\n",
        "turtles.split()"
      ],
      "metadata": {
        "id": "j2pTkAurmO82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, we might want to perform some operations on this text *before* we start processing it for linguistic information. These operations will work to normalize and standardize the text so that noise is removed. This is called preprocessing. Preprocessing comes in many options - you could remove just punctuation, or convert everything to lowercase, or remove very frequent words, or remove words that are not in the dictionary, or remove words that only occur one time, and so on. Different algorithms and approaches to NLP will all include their own methods and steps for preprocessing, which are tied to the goals of the analysis.\n",
        "\n",
        "For now, let's focus on the issue of punctuation in the turtles text."
      ],
      "metadata": {
        "id": "2AKrsYJGPYZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning punctuation**\n",
        "\n",
        "Pur problem above with `turtles` was caused by the use of punctuation and `.split()`. What could we do? Well, we *could* remove all of the punctuation before splitting the text, and this would provide a satisfactory solution (for now).\n",
        "\n",
        "Based on what we know now about Python, how could we remove all of the punctuation from a text? We can actually do this quite simply and quickly using a list comprehension.\n",
        "\n",
        "We would want to set up a conditional test which inspects each character in a string, and as long as that character is *not* a punctuation mark, keep it.\n",
        "\n",
        "Here is some pseudocode that expresses our goal:\n",
        "\n",
        "\n",
        "```\n",
        "[character for character in string if character not punctuation]\n",
        "```\n",
        "\n",
        "To execute this code, we'd need to tell Python what we mean by \"punctuation\". One way is to define a string containing all the punctuation marks we don't want.\n",
        "\n",
        "We will also lower case the strings within the same expression.\n"
      ],
      "metadata": {
        "id": "ZSFw1qyDnMn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation we don't like, in this case just commas and exclamation marks\n",
        "punctuation = ',!'"
      ],
      "metadata": {
        "id": "2UnRKy0vsIXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run the cell below, you still see that the punctuation has been removed, but unfortunately the output is a list of characters, not words!"
      ],
      "metadata": {
        "id": "giwzMHZCj3lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write a list comprehension that only keeps characters that aren't in punctuation\n",
        "[character.lower() for character in turtles if character not in punctuation]"
      ],
      "metadata": {
        "id": "voRnrvuQsM9F",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`string.join()`**\n",
        "\n",
        "The list comprehension has returned a list of *characters*, but we wanted to retain the whitespace and other properties of the texts as a series of words. No worries, we can use the handy `string.join()` function to join a list of characters back into one string!\n",
        "\n",
        "`string.join()` is sort of the bizzare cousin of `string.split()`. Because `.join()` is actually a string method, you need to attach a string to the front part of the `.join()`. The string that you attach to `.join` represents the nature of the join...the character that you want to join everything by. Much like `.split()`, you can choose whatever you like to join stuff with.\n",
        "\n",
        "For example `''.join()` will join using an empty string, `' '.join()` will join usinga space, and `'HELLO'.join()` would join everything with the string `HELLO`.\n",
        "\n",
        "It is a bit confusing at first, but basically we use the part in front of `.join()` to determine *how* the characters should be glued back together.\n"
      ],
      "metadata": {
        "id": "7adt3Qg4sWpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = ['n', 'i','n','j','a',' ', 't','u','r','t', 'l', 'e']"
      ],
      "metadata": {
        "id": "81dvZGR0juuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join with an empty string\n",
        "''.join(example_string)"
      ],
      "metadata": {
        "id": "Heh7Fz8Kj_7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join with a space\n",
        "' '.join(example_string)"
      ],
      "metadata": {
        "id": "Av-sHb_pkWa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join with HELLO\n",
        "'HELLO'.join(example_string)"
      ],
      "metadata": {
        "id": "PQD59ZLAjlF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, if we simply wanted to glue back together a list of characters *without* making any other changes, we would then attach an empty string to `.join()`, indicated with two string delimiters: `''`, in which case we would type `''.join()`.\n",
        "\n",
        "Then, the thing that you want to join goes inside the `()` part of `''.join()`.\n",
        "\n",
        "```\n",
        "''.join([list of characters])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "24B_MCcqkftE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we just wrap the whole list comprehension in ''.join\n",
        "remove_punctuation = ''.join([character.lower() for character in turtles if character not in punctuation])"
      ],
      "metadata": {
        "id": "qRnGOhFxsjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it looks different now...but it's been reformed back into what we first had without punctuation\n",
        "remove_punctuation"
      ],
      "metadata": {
        "id": "-XV-oJJvs6A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How else could we rejoin our cleaned text without using `.join()`?\n",
        "\n",
        "One way would be to write a loop which analyses each word in a text, removing punctuation from that word, and then puts that word into a list. This is made slightly difficult because strings are `immutable`, meaning that we cannot remove or replace individual elements of a string.\n"
      ],
      "metadata": {
        "id": "tUA2RIaxRF8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this returns an error because we cannot modify strings in place\n",
        "'string'[0] = 'b'"
      ],
      "metadata": {
        "id": "JRshUaXIRZuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to do this is scan through each character and then reconstruct the string as we go, only including characters that pass a conditional test. This method iteratively creates a new string by first creating an empty string `output` and then adding each character to it in a sequence using string concatenation.\n",
        "\n"
      ],
      "metadata": {
        "id": "53E7M730RgKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an output container\n",
        "output = ''\n",
        "\n",
        "# loop through each character in the whole string\n",
        "for character in turtles:\n",
        "  # if the character is NOT in this list:\n",
        "  if character not in [',', '!']:\n",
        "    # add the lowercased version of the character to the list\n",
        "    output = output + character.lower()\n",
        "\n",
        "# results are identical to the ''.join() method above\n",
        "output"
      ],
      "metadata": {
        "id": "3o_zzqtlRfgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### using a regular expression\n",
        "\n",
        "Another way is to use a `regular expression` to clean the string. A regular expression is a method of defining complex string patterns using abstract symbols. Using regular expressions, we can quickly search and replace strings for specific patterns. Here we will use a simple pattern and a function to replace the pattern.\n",
        "\n",
        "We will need to import the library for regular expressions, `re`"
      ],
      "metadata": {
        "id": "WgfJp41EkTcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the regular expression library\n",
        "import re"
      ],
      "metadata": {
        "id": "1mQ_oI3rkiB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the `re.sub` function, which will substitute patterns in a string with another pattern. The syntax for `re.sub` is:\n",
        "\n",
        "`re.sub(pattern, replacement, string)`\n",
        "\n",
        "So you first type the pattern that you want to search for, then what you would like the pattern replaced with, and then finally the string that you are targeting - you are telling `re.sub` to replace THIS with THAT over THERE.\n",
        "\n",
        "And, if we say that the replacement should be an empty string, then the replacement will be nothing, meaning that you are effectively removing the pattern from the string. For example:"
      ],
      "metadata": {
        "id": "iR2JD_eKki0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all the 'a' from the string 'banana'\n",
        "re.sub(pattern = 'a', repl = '', string = 'banana')"
      ],
      "metadata": {
        "id": "xgzvRm-Ak6uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this same logic, we can remove all of the punctuation from a string. Be sure to save the results as a variable, otherwise the replacements will not be saved.\n"
      ],
      "metadata": {
        "id": "xEGiyy6HlaWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# original string\n",
        "exclamation = 'too! many! exclamation! points!'\n",
        "exclamation"
      ],
      "metadata": {
        "id": "-n32ad-qlhMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# substitute out the exclamation marks and make a new string\n",
        "exclamation = re.sub(pattern = '!', repl = '', string = exclamation)"
      ],
      "metadata": {
        "id": "F8PgIKa5l1bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a cleaned string\n",
        "exclamation"
      ],
      "metadata": {
        "id": "T8aR3q2Tl0wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we want to remove more than one punctuation mark, we can define a pattern which says \"anything in this pattern.\" To do so, write a string with brackets and put any character you want removed in those brackets, like this:\n",
        "\n",
        "```\n",
        "punctuation = [',!']\n",
        "```\n",
        "\n",
        "Then use that pattern in your `re.sub` call to replace those punctuation marks."
      ],
      "metadata": {
        "id": "-PjvocaumDX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# original version of turtles\n",
        "turtles"
      ],
      "metadata": {
        "id": "Omv808NqmZFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned version of turtles (not saved to a variable)\n",
        "punctuation = '[,!]'\n",
        "re.sub(pattern = punctuation, repl = '', string = turtles)"
      ],
      "metadata": {
        "id": "XiNW7UcPmcGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using `nltk.word_tokenize()` for better tokenization\n",
        "\n",
        "Now we understand how to preprocess text so that `.split()` will return a more normalised set of tokens.\n",
        "\n",
        "Having learned this, we now need to ask, what if we want to *retain* punctuation? Do you think it would be important to know the difference between words that come before / after punctuation? Could punctuation tell us something about the syntax of a sentence or the tone of voice of writing? These are questions without clear answers, but are worthy of consideration. Another more practical aspect of retaining punctuation is that punctuation markers could help with segmentation of strings into words and/or sentences. For this reason, we will actually stop using `.split()` as a means to create word tokens, and moreover think about whether punctuation is needed.\n",
        "\n",
        "> Please keep in mind that learning how to clean the strings and using `.split()` is still useful, so it was not for naught. You may still find that using `.split()` and some cleaning is helpful for various subtasks you might want to perform.\n",
        "\n",
        "Anyhow, let's look at the NLTK segmentation functions which are improvements upon `.split()`. These function are `nltk.word_tokenize()` and `nltk.sent_tokenize()`. They convert raw strings into tokens or sentences, respectively.\n"
      ],
      "metadata": {
        "id": "JZMa2C36T9ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First import the NLTK library and download the necessary `punkt` resource. The `punkt` resource is the algoritm that NLTK uses to identify sentence and word boundaries.\n",
        "\n",
        "Edit: as of mid-2024, this has been replaced with [`punkt_tab`](https://github.com/nltk/nltk/issues/3293)"
      ],
      "metadata": {
        "id": "95coTKOoovlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Idmz3YEyofpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize with NLTK, we use the `nltk.word_tokenize()` function with a string as input.\n",
        "\n",
        "In the cells below, compare the difference between using `.split()` and `nltk.word_tokenize()`:"
      ],
      "metadata": {
        "id": "jfZ9vVFbotdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between using `.split()` and `nltk.word_tokenize()`?\n",
        "pretzels = 'These pretzels are making me thirsty!'\n",
        "\n",
        "split_tokens = pretzels.split()\n",
        "nltk_tokens = nltk.word_tokenize(pretzels)\n",
        "\n",
        "print(f\"Using .split(): \\n{split_tokens}\\n\\nUsing nltk: \\n{nltk_tokens}\")"
      ],
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK tokenizer has treated the punctuation as a separate word - so it is smart enough to recognise that words should be separated from punctuation. It does this using a set of additional rules as well as some splitting. This makes perfect sense for punctuation which occurs after words, such as commas, full stops, exclamation marks, and so on.\n",
        "\n",
        "What's going on in the cell below?"
      ],
      "metadata": {
        "id": "dKVnaWihWB7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is different about these tokens?\n",
        "nltk.word_tokenize('I can\\'t even.')"
      ],
      "metadata": {
        "id": "7Z_sax5nWL7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word \"can't\" was split into two tokens! Why is that? Well, if we think about it, \"can't\" actually stands for *two* words - \"can\" and \"not.\" The tokenizer has an additional set of rules to search these contractions and split them accordingly. Using `.split()`, on the other hand, would result in \"can't\" being stored as a single word. Moreover, removing the punctuation *before* tokenization would turn \"can't\" into \"cant\", and then `nltk.word_tokenize()` would treat \"cant\" as a single word. Is this an issue? Well, considering the word \"cant\" is its own word separate in meaning from \"can't\", it certainly could be.\n",
        "\n",
        "\n",
        "The point is that the order of pre-processing and normalisation steps is important, as are the different things you might want to do to a text. Many modern NLP libraries perform pre-processing automatically, and some analyses now actually do not need any preprocessing at all! It is nonetheless fundamental to understand how your data is being normalised in order to use these functions properly.\n",
        "\n",
        "As a general rule, using `nltk.word_tokenize()` is preferred to `.split()`, because with `word_tokenize()` you retain the punctuation as separate tokens, which you can then choose to use or not use in your analysis."
      ],
      "metadata": {
        "id": "JOr4HBbHWa0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating sentences with `nltk.sent_tokenize()`**\n",
        "\n",
        "We can also obtain full sentences from texts using the `nltk.sent_tokenize()` function, which operates in the same way. The output here is a list of sentences."
      ],
      "metadata": {
        "id": "U632YZX9poYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all the sentences in turtles\n",
        "nltk.sent_tokenize(turtles)"
      ],
      "metadata": {
        "id": "Tp-T1zbapxnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The turtles text was already organised by newlines - here is a two sentence string, showing how `sent_tokenize()` also addresses this:"
      ],
      "metadata": {
        "id": "GB9_tiDEqJVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a string not separated by newlines is still split into sentences.\n",
        "nltk.sent_tokenize(\"Give a man a fire and he's warm for a day. Set fire to him and he\\'s warm for the rest of his life.\")"
      ],
      "metadata": {
        "id": "5SX7Enk5p5gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see how reliant it is on punctuation:\n",
        "nltk.sent_tokenize(\"Give a man a fire and he's warm for a day Set fire to him and he\\'s warm for the rest of his life.\")"
      ],
      "metadata": {
        "id": "dxCkViykqU1i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}